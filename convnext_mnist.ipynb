{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VuHuyBui/convnext-mnist-with-flax/blob/main/convnext_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flax -q"
      ],
      "metadata": {
        "id": "VADVH9F8ZnGo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96f84352-abb3-44c2-9efe-2ef544f6d95f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 180 kB 30.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 217 kB 59.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 145 kB 70.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 51 kB 6.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 85 kB 4.1 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dI6_oVeXcBGS"
      },
      "outputs": [],
      "source": [
        "import jax \n",
        "from jax import numpy as jnp, random    # JAX NumPy\n",
        "\n",
        "from flax import linen as nn           # The Linen API\n",
        "from flax.training import train_state  # Useful dataclass to keep train state\n",
        "\n",
        "import numpy as np                     # Ordinary NumPy\n",
        "import optax                           # Optimizers\n",
        "import tensorflow_datasets as tfds\n",
        "from tqdm import tqdm\n",
        "\n",
        "import functools\n",
        "from typing import Sequence, Tuple, Any"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a97FRzonxb2F"
      },
      "source": [
        "## 2. Define our *ConvNext* with *Flax*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7_Qla-b9r-gS"
      },
      "outputs": [],
      "source": [
        "class ConvNextBlock(nn.Module):\n",
        "    filters: int\n",
        "    kernel_size: Tuple[int, int] = (7, 7)\n",
        "    strides: Tuple[int, int] = (1, 1)\n",
        "    act: Any = jnp.add\n",
        "    dtype: Any = jnp.float32\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        res = x\n",
        "        x = nn.Conv(self.filters, self.kernel_size, strides=self.strides, feature_group_count=self.filters)(x)\n",
        "        x = nn.LayerNorm(dtype=self.dtype)(x)\n",
        "        x = nn.Conv(self.filters * 4, kernel_size=(1, 1), strides=self.strides)(x)\n",
        "        x = nn.gelu(x)\n",
        "        x = nn.Conv(self.filters, self.kernel_size, strides=(1, 1))(x)\n",
        "\n",
        "        return self.act(res, x)\n",
        "\n",
        "class ConvNextDownsamplingBlock(nn.Module):\n",
        "    filters: int\n",
        "    kernel_size: Tuple[int, int] = (2, 2)\n",
        "    strides: Tuple[int, int] = (2, 2)\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = nn.LayerNorm()(x)\n",
        "        x = nn.Conv(self.filters, strides=self.strides, kernel_size=self.kernel_size)(x)\n",
        "        return x\n",
        "\n",
        "class ConvNext(nn.Module):\n",
        "    stage_size: Sequence[int]\n",
        "    channels: Sequence[int]\n",
        "    dtype: Any = jnp.float32\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = nn.Conv(self.channels[0], kernel_size=(4, 4), strides=(4, 4))(x)\n",
        "        x = nn.LayerNorm(dtype=self.dtype)(x)\n",
        "        for i, channel in enumerate(self.channels):\n",
        "            for j in range(self.stage_size[i]):\n",
        "                x = ConvNextBlock(channel)(x)\n",
        "            if i <= len(self.channels) - 2:\n",
        "                x = ConvNextDownsamplingBlock(self.channels[i+1])(x)\n",
        "        x = nn.LayerNorm(dtype=self.dtype)(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ClassisficationHead(nn.Module):\n",
        "    num_class: int\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = x.reshape((x.shape[0], -1))  # flatten\n",
        "        x = nn.Dense(features=256)(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.Dense(features=10)(x)\n",
        "        return x\n",
        "\n",
        "class ConvNextForClassification(nn.Module):\n",
        "    num_class: int\n",
        "    stage_size: Sequence[int] = (1, 1, 3, 1)\n",
        "    channels: Sequence[int] = (96, 192, 384, 768)\n",
        "    dtype: Any = jnp.float32\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = ConvNext(stage_size=self.stage_size, channels=self.channels, dtype=self.dtype)(x)\n",
        "        x = ClassisficationHead(num_class=self.num_class)(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDEoAprU6_JZ"
      },
      "source": [
        "## 3. Define loss\n",
        "\n",
        "We simply use `optax.softmax_cross_entropy()`. Note that this function expects both `logits` and `labels` to have shape `[batch, num_classes]`. Since the labels will be read from TFDS as integer values, we first need to convert them to a onehot encoding.\n",
        "\n",
        "Our function returns a simple scalar value ready for optimization, so we first take the mean of the vector shaped `[batch]` returned by Optax's loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Zcb_ebU87G7s"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(*, logits, labels):\n",
        "  labels_onehot = jax.nn.one_hot(labels, num_classes=10)\n",
        "  return optax.softmax_cross_entropy(logits=logits, labels=labels_onehot).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INZE3eM67JUr"
      },
      "source": [
        "## 4. Metric computation\n",
        "\n",
        "For loss and accuracy metrics, create a separate function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KvuEA8Tw-MYa"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(*, logits, labels):\n",
        "  loss = cross_entropy_loss(logits=logits, labels=labels)\n",
        "  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
        "  metrics = {\n",
        "      'loss': loss,\n",
        "      'accuracy': accuracy,\n",
        "  }\n",
        "  return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYz0Emry-ele"
      },
      "source": [
        "## 5. Loading data\n",
        "\n",
        "Define a function that loads and prepares the MNIST dataset and converts the\n",
        "samples to floating-point numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IOeWiS_b-p8O"
      },
      "outputs": [],
      "source": [
        "def get_datasets():\n",
        "  \"\"\"Load MNIST train and test datasets into memory.\"\"\"\n",
        "  ds_builder = tfds.builder('mnist')\n",
        "  ds_builder.download_and_prepare()\n",
        "  train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))\n",
        "  test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))\n",
        "  train_ds['image'] = jnp.float32(train_ds['image']) / 255.\n",
        "  test_ds['image'] = jnp.float32(test_ds['image']) / 255.\n",
        "  return train_ds, test_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMFK51rsAUX4"
      },
      "source": [
        "## 6. Create train state\n",
        "\n",
        "A common pattern in Flax is to create a single dataclass that represents the\n",
        "entire training state, including step number, parameters, and optimizer state.\n",
        "\n",
        "Also adding optimizer & model to this state has the advantage that we only need\n",
        "to pass around a single argument to functions like `train_step()` (see below).\n",
        "\n",
        "Because this is such a common pattern, Flax provides the class\n",
        "[flax.training.train_state.TrainState](https://flax.readthedocs.io/en/latest/flax.training.html#train-state)\n",
        "that serves most basic usecases. Usually one would subclass it to add more data\n",
        "to be tracked, but in this example we can use it without any modifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QadyBPbWBEAT"
      },
      "outputs": [],
      "source": [
        "def create_train_state(rng, learning_rate, momentum):\n",
        "  \"\"\"Creates initial `TrainState`.\"\"\"\n",
        "  cnn = ConvNextForClassification(10)\n",
        "  params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']\n",
        "  tx = optax.sgd(learning_rate, momentum)\n",
        "  return train_state.TrainState.create(\n",
        "      apply_fn=cnn.apply, params=params, tx=tx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7l-75YE-sr-"
      },
      "source": [
        "## 7. Training step\n",
        "\n",
        "A function that:\n",
        "\n",
        "- Evaluates the neural network given the parameters and a batch of input images\n",
        "  with the\n",
        "  [Module.apply](https://flax.readthedocs.io/en/latest/flax.linen.html#flax.linen.Module.apply)\n",
        "  method.\n",
        "- Computes the `cross_entropy_loss` loss function.\n",
        "- Evaluates the loss function and its gradient using\n",
        "  [jax.value_and_grad](https://jax.readthedocs.io/en/latest/jax.html#jax.value_and_grad).\n",
        "- Applies a\n",
        "  [pytree](https://jax.readthedocs.io/en/latest/pytrees.html#pytrees-and-jax-functions)\n",
        "  of gradients to the optimizer to update the model's parameters.\n",
        "- Computes the metrics using `compute_metrics` (defined earlier).\n",
        "\n",
        "Use JAX's [@jit](https://jax.readthedocs.io/en/latest/jax.html#jax.jit)\n",
        "decorator to trace the entire `train_step` function and just-in-time compile\n",
        "it with [XLA](https://www.tensorflow.org/xla) into fused device operations\n",
        "that run faster and more efficiently on hardware accelerators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ng11cdMf-z0x"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def train_step(state, batch):\n",
        "  \"\"\"Train for a single step.\"\"\"\n",
        "  def loss_fn(params):\n",
        "    logits = ConvNextForClassification(10).apply({'params': params}, batch['image'])\n",
        "    loss = cross_entropy_loss(logits=logits, labels=batch['label'])\n",
        "    return loss, logits\n",
        "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "  (_, logits), grads = grad_fn(state.params)\n",
        "  state = state.apply_gradients(grads=grads)\n",
        "  metrics = compute_metrics(logits=logits, labels=batch['label'])\n",
        "  return state, metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-4qDNUgBryr"
      },
      "source": [
        "## 8. Evaluation step\n",
        "\n",
        "Create a function that evaluates your model on the test set with\n",
        "[Module.apply](https://flax.readthedocs.io/en/latest/flax.linen.html#flax.linen.Module.apply)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "w1J9i6alBv_u"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def eval_step(params, batch):\n",
        "  logits = ConvNextForClassification(10).apply({'params': params}, batch['image'])\n",
        "  return compute_metrics(logits=logits, labels=batch['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBTLQPC4BxgH"
      },
      "source": [
        "## 9. Train function\n",
        "\n",
        "Define a training function that:\n",
        "\n",
        "- Shuffles the training data before each epoch using\n",
        "  [jax.random.permutation](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.permutation.html)\n",
        "  that takes a PRNGKey as a parameter (check the\n",
        "  [JAX - the sharp bits](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#JAX-PRNG)).\n",
        "- Runs an optimization step for each batch.\n",
        "- Retrieves the training metrics from the device with `jax.device_get` and\n",
        "  computes their mean across each batch in an epoch.\n",
        "- Returns the optimizer with updated parameters and the training loss and\n",
        "  accuracy metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7ipyJ-JGCNqP"
      },
      "outputs": [],
      "source": [
        "def train_epoch(state, train_ds, batch_size, epoch, rng):\n",
        "  \"\"\"Train for a single epoch.\"\"\"\n",
        "  train_ds_size = len(train_ds['image'])\n",
        "  steps_per_epoch = train_ds_size // batch_size\n",
        "\n",
        "  perms = jax.random.permutation(rng, train_ds_size)\n",
        "  perms = perms[:steps_per_epoch * batch_size]  # skip incomplete batch\n",
        "  perms = perms.reshape((steps_per_epoch, batch_size))\n",
        "  batch_metrics = []\n",
        "  for perm in perms:\n",
        "    batch = {k: v[perm, ...] for k, v in train_ds.items()}\n",
        "    state, metrics = train_step(state, batch)\n",
        "    batch_metrics.append(metrics)\n",
        "\n",
        "  # compute mean of metrics across each batch in epoch.\n",
        "  batch_metrics_np = jax.device_get(batch_metrics)\n",
        "  epoch_metrics_np = {\n",
        "      k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
        "      for k in batch_metrics_np[0]}\n",
        "\n",
        "  print('train epoch: %d, loss: %.4f, accuracy: %.2f' % (\n",
        "      epoch, epoch_metrics_np['loss'], epoch_metrics_np['accuracy'] * 100))\n",
        "\n",
        "  return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2cHbVUfCRMv"
      },
      "source": [
        "## 10. Eval function\n",
        "\n",
        "Create a model evaluation function that:\n",
        "\n",
        "- Retrieves the evaluation metrics from the device with `jax.device_get`.\n",
        "- Copies the metrics\n",
        "  [data stored](https://flax.readthedocs.io/en/latest/design_notes/linen_design_principles.html#how-are-parameters-represented-and-how-do-we-handle-general-differentiable-algorithms-that-update-stateful-variables)\n",
        "  in a JAX\n",
        "  [pytree](https://jax.readthedocs.io/en/latest/pytrees.html#pytrees-and-jax-functions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_dKahNmMCr5q"
      },
      "outputs": [],
      "source": [
        "def eval_model(params, test_ds):\n",
        "  metrics = eval_step(params, test_ds)\n",
        "  metrics = jax.device_get(metrics)\n",
        "  summary = jax.tree_util.tree_map(lambda x: x.item(), metrics)\n",
        "  return summary['loss'], summary['accuracy']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHQi20yVCsSf"
      },
      "source": [
        "## 11. Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6CLXnP3KHptR"
      },
      "outputs": [],
      "source": [
        "train_ds, test_ds = get_datasets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56rKPl6OHqu8"
      },
      "source": [
        "## 12. Seed randomness\n",
        "\n",
        "- Get one\n",
        "  [PRNGKey](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.PRNGKey.html#jax.random.PRNGKey)\n",
        "  and\n",
        "  [split](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.split.html#jax.random.split)\n",
        "  it to get a second key that you'll use for parameter initialization. (Learn\n",
        "  more about\n",
        "  [PRNG chains](https://flax.readthedocs.io/en/latest/design_notes/linen_design_principles.html#how-are-parameters-represented-and-how-do-we-handle-general-differentiable-algorithms-that-update-stateful-variables)\n",
        "  and\n",
        "  [JAX PRNG design](https://github.com/google/jax/blob/main/design_notes/prng.md).)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "n53xh_B8Ht_W"
      },
      "outputs": [],
      "source": [
        "rng = jax.random.PRNGKey(0)\n",
        "rng, init_rng = jax.random.split(rng)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3iHFiAuH41s"
      },
      "source": [
        "## 13. Initialize train state\n",
        "\n",
        "Remember that function initializes both the model parameters and the optimizer\n",
        "and puts both into the training state dataclass that is returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Mj6OfdEEIU-o"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.1\n",
        "momentum = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_87fL90dH-0Z"
      },
      "outputs": [],
      "source": [
        "state = create_train_state(init_rng, learning_rate, momentum)\n",
        "del init_rng  # Must not be used anymore."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqNrWu7kIC9S"
      },
      "source": [
        "## 14. Train and evaluate\n",
        "\n",
        "Once the training and testing is done after 10 epochs, the output should show that your model was able to achieve approximately 99% accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0nxgS5Z5IsT_"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "batch_size = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugGlV3u6Iq1A"
      },
      "outputs": [],
      "source": [
        "for epoch in range(1, num_epochs + 1):\n",
        "  # Use a separate PRNG key to permute image data during shuffling\n",
        "  rng, input_rng = jax.random.split(rng)\n",
        "  # Run an optimization step over a training batch\n",
        "  state = train_epoch(state, train_ds, batch_size, epoch, input_rng)\n",
        "  # Evaluate on the test set after each training epoch \n",
        "  test_loss, test_accuracy = eval_model(state.params, test_ds)\n",
        "  print(' test epoch: %d, loss: %.2f, accuracy: %.2f' % (\n",
        "      epoch, test_loss, test_accuracy * 100))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "i8gxbQrkxTlN"
      ],
      "provenance": [],
      "mount_file_id": "1mYLpGle90pxXSwKujoWkZHiSl7mGbTqg",
      "authorship_tag": "ABX9TyNZiNTjasSOgw4zs7IhsoiD",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}